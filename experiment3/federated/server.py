# =============================================================================
# federated/server.py
# =============================================================================

# federated/server.py
import torch
import numpy as np
from .client import FederatedClient
from .aggregator import FedAvgAggregator


class FederatedServer:
    def __init__(self, fed_dataset, config, attacker_class):
        self.config = config
        self.fed_dataset = fed_dataset
        self.attacker_class = attacker_class
        self.aggregator = FedAvgAggregator()

        # ğŸ”§ ä¿®å¤ï¼šå…ˆåˆå§‹åŒ–attack_stats
        self.attack_stats = {
            'malicious_clients': [],
            'attack_type': config.attack_type,
            'total_rounds': 0
        }

        # ç„¶ååˆ›å»ºå®¢æˆ·ç«¯ï¼ˆä¼šç”¨åˆ°attack_statsï¼‰
        self.clients = self._create_clients()
        self.global_model = self._init_global_model()
        self.model_history = []

    def _create_clients(self):
        clients = []
        malicious_ids = np.random.choice(self.config.num_clients,
                                         self.config.num_malicious, replace=False)

        print(f"æ¶æ„å®¢æˆ·ç«¯ID: {malicious_ids}")
        print(f"æ”»å‡»ç±»å‹: {self.config.attack_type}")

        for i in range(self.config.num_clients):
            dataset = self.fed_dataset.get_client_data(i)
            is_malicious = i in malicious_ids

            # ä¸ºæ¯ä¸ªæ¶æ„å®¢æˆ·ç«¯åˆ›å»ºä¸“é—¨çš„æ”»å‡»å™¨
            attacker = None
            if is_malicious:
                if self.config.attack_type == "distributed":
                    # åˆ†å¸ƒå¼æ”»å‡»éœ€è¦ä¼ é€’å®¢æˆ·ç«¯ID
                    from attack.backdoor import DistributedBackdoorAttack
                    attacker = DistributedBackdoorAttack(self.config, client_id=i)
                else:
                    # å…¶ä»–æ”»å‡»ç±»å‹
                    attacker = self.attacker_class(self.config)

                self.attack_stats['malicious_clients'].append(i)

            client = FederatedClient(i, dataset, self.config, is_malicious, attacker)
            clients.append(client)

        return clients

    def _init_global_model(self):
        from models.resnet import ResNet20
        model = ResNet20(self.config.num_classes)
        return model.state_dict()

    def train(self):
        print(f"å¼€å§‹{self.config.attack_type}æ”»å‡»çš„è”é‚¦å­¦ä¹ è®­ç»ƒ...")

        for round_num in range(self.config.num_rounds):
            # é€‰æ‹©å®¢æˆ·ç«¯
            selected_clients = np.random.choice(self.clients,
                                                self.config.clients_per_round, replace=False)

            # ç»Ÿè®¡æœ¬è½®æ¶æ„å®¢æˆ·ç«¯
            round_malicious = sum(1 for client in selected_clients if client.is_malicious)

            # æ”¶é›†æœ¬åœ°æ›´æ–°
            client_models = []
            for client in selected_clients:
                model_state = client.local_train(self.global_model)
                client_models.append({
                    'model': model_state,
                    'is_malicious': client.is_malicious,
                    'client_id': client.client_id,
                    'attack_type': self.config.attack_type if client.is_malicious else None
                })

            # åº”ç”¨æ¨¡å‹æ›¿æ¢æ”»å‡»ï¼ˆé’ˆå¯¹æ¶æ„å®¢æˆ·ç«¯ï¼‰
            for client_info in client_models:
                if client_info['is_malicious']:
                    client_info['model'] = self._apply_enhanced_model_replacement(
                        client_info['model'], client_info['attack_type']
                    )

            # èšåˆæ¨¡å‹
            model_states = [info['model'] for info in client_models]
            self.global_model = self.aggregator.aggregate(model_states)

            # ä¿å­˜æ¨¡å‹å†å²
            self.model_history.append(client_models)
            self.attack_stats['total_rounds'] += 1

            if (round_num + 1) % 10 == 0:
                print(f"Round {round_num + 1}/{self.config.num_rounds} completed")
                print(f"  æœ¬è½®æ¶æ„å®¢æˆ·ç«¯æ•°: {round_malicious}/{len(selected_clients)}")

                # ä¸­æœŸè¯„ä¼°
                if (round_num + 1) % 20 == 0:
                    self._mid_training_evaluation(round_num + 1)

    def _apply_enhanced_model_replacement(self, model_state, attack_type):
        """å¢å¼ºçš„æ¨¡å‹æ›¿æ¢æ”»å‡»"""
        scaled_state = {}

        # æ ¹æ®æ”»å‡»ç±»å‹è°ƒæ•´ç¼©æ”¾ç­–ç•¥
        if attack_type == "distributed":
            # åˆ†å¸ƒå¼æ”»å‡»ä½¿ç”¨è¾ƒå°çš„ç¼©æ”¾
            scale = self.config.scale_factor * 0.7
        elif attack_type == "edge_case":
            # è¾¹ç¼˜æ¡ˆä¾‹æ”»å‡»ä½¿ç”¨æ›´å¤§çš„ç¼©æ”¾
            scale = self.config.scale_factor * 1.3
        else:
            # å•æºæ”»å‡»ä½¿ç”¨æ ‡å‡†ç¼©æ”¾
            scale = self.config.scale_factor

        for key, value in model_state.items():
            if value.dtype in [torch.float32, torch.float64]:
                # å¯¹ä¸åŒå±‚ä½¿ç”¨ä¸åŒçš„ç¼©æ”¾ç­–ç•¥
                if 'conv' in key or 'linear' in key:
                    # å¯¹å…³é”®å±‚ä½¿ç”¨æ›´å¼ºçš„ç¼©æ”¾
                    scaled_state[key] = value * scale
                else:
                    # å¯¹BNå±‚ç­‰ä½¿ç”¨è¾ƒå°çš„ç¼©æ”¾
                    scaled_state[key] = value * (scale * 0.5)
            else:
                # æ•´æ•°ç±»å‹å‚æ•°ä¿æŒä¸å˜
                scaled_state[key] = value

        return scaled_state

    def _mid_training_evaluation(self, round_num):
        """ä¸­æœŸè®­ç»ƒè¯„ä¼°"""
        print(f"  Round {round_num} ä¸­æœŸè¯„ä¼°å®Œæˆ")

    def get_models_for_analysis(self):
        benign_models = []
        malicious_models = []

        for round_models in self.model_history:
            for client_info in round_models:
                from models.resnet import ResNet20
                model = ResNet20(self.config.num_classes)
                model.load_state_dict(client_info['model'])

                if client_info['is_malicious']:
                    malicious_models.append(model)
                else:
                    benign_models.append(model)

        return benign_models, malicious_models

    def get_attack_statistics(self):
        """è·å–æ”»å‡»ç»Ÿè®¡ä¿¡æ¯"""
        return {
            'attack_type': self.attack_stats['attack_type'],
            'malicious_clients': self.attack_stats['malicious_clients'],
            'total_rounds': self.attack_stats['total_rounds'],
            'malicious_participation': len(self.attack_stats['malicious_clients']) / self.config.num_clients
        }
